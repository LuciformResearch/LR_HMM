/**
 * Agent de r√©sum√© pour LR_TchatAgent Web
 * Cr√©e des r√©sum√©s narratifs de l'historique des conversations pour la compression m√©moire.
 * Bas√© sur le syst√®me Python existant.
 */

import { UnifiedProvider, UnifiedProviderFactory } from '@/lib/providers/UnifiedProvider';
import fetch from 'node-fetch';
import { GoogleAuth } from 'google-auth-library';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { getSimplePrompts } from './SimplePrompts';

export interface SummarizationConfig {
  provider: string;
  model: string;
  consciousnessLevel: number;
  persona?: any;
  maxSummaryLength: number;
  minSummaryLength?: number;
  lengthEmphasis?: boolean; // strengthen length instruction
  strict?: boolean; // if true, no local fallback
  timeoutMs?: number; // timeout for model calls
  // Vertex options
  useVertex?: boolean;
  project?: string;
  location?: string;
  maxOutputTokens?: number;
}

export interface ConversationMessage {
  role: 'user' | 'assistant';
  content: string;
  timestamp: string;
  metadata?: any;
}

export interface SummaryResult {
  summary: string;
  compressionRatio: number;
  qualityScore: number;
  metadata: {
    originalMessages: number;
    summaryLength: number;
    processingTime: number;
  };
}

export class SummarizationAgent {
  private config: SummarizationConfig;
  private provider: any;
  private googleModel: any | null = null;
  private useVertex = false;

  constructor(config: Partial<SummarizationConfig> = {}) {
    this.config = {
      provider: 'gemini',
      model: 'gemini-1.5-flash',
      consciousnessLevel: 0.6,
      maxSummaryLength: 200,
      ...config
    };

    this.useVertex = !!config.useVertex || String(process.env.VERTEX_L1 || '').toLowerCase() === 'true';
    this.initializeProvider().catch(error => {
      console.error('‚ùå Erreur initialisation SummarizationAgent:', error);
    });
  }

  private async initializeProvider(): Promise<void> {
    try {
      if (this.useVertex) {
        console.log('‚úÖ SummarizationAgent: Vertex AI mode activ√©');
        this.googleModel = null; // ensure we don't use AI Studio path when Vertex requested
      }
      // 1) Essayer provider via environnement (scripts / serveur)
      const geminiKey = process.env.GEMINI_API_KEY;
      const model = process.env.GEMINI_MODEL || this.config.model || 'gemini-1.5-flash';
      if (!this.useVertex && geminiKey && geminiKey.length > 10) {
        // Pr√©f√©rer le client officiel Google pour fiabilit√©
        try {
          const genAI = new GoogleGenerativeAI(geminiKey);
          this.googleModel = genAI.getGenerativeModel({ model });
          console.log(`‚úÖ SummarizationAgent: GoogleGenerativeAI pr√™t (${model})`);
        } catch (e) {
          console.warn('‚ö†Ô∏è √âchec initialisation GoogleGenerativeAI, fallback UnifiedProvider:', e);
          this.googleModel = null;
        }
        this.provider = UnifiedProviderFactory.create({
          type: 'custom',
          provider: 'gemini',
          model,
          apiKey: geminiKey
        });
        if (this.provider && this.provider.isAvailable()) {
          console.log(`‚úÖ SummarizationAgent: provider Gemini via .env (${model})`);
          return;
        }
      }

      // 2) Sinon, tenter un provider via base (client/browser)
      if (!this.useVertex) {
        this.provider = await UnifiedProviderFactory.createFromDatabase('system');
      }
      if (this.provider && this.provider.isAvailable()) {
        console.log(`‚úÖ SummarizationAgent initialis√© via DB (${this.config.provider}:${this.config.model})`);
      } else {
        console.warn(`‚ö†Ô∏è Aucun provider disponible (.env/DB). Fallback local sera utilis√©.`);
        this.provider = null;
      }
    } catch (error) {
      console.error(`‚ùå Erreur initialisation provider: ${error}`);
      this.provider = null;
    }
  }

  /**
   * Cr√©e un r√©sum√© narratif de la conversation
   */
  async summarizeConversation(
    messages: ConversationMessage[],
    interlocutor: string = 'user',
    language: string = 'fr'
  ): Promise<SummaryResult> {
    const startTime = Date.now();

    if (this.useVertex) {
      return this.vertexSummarize(messages, interlocutor, language, startTime);
    }

    if (!this.provider && !this.googleModel) {
      if (this.config.strict) {
        throw new Error('Provider indisponible et mode strict activ√©');
      }
      return this.fallbackSummary(messages, interlocutor, startTime);
    }

    try {
      // Construire le prompt de r√©sum√©
      const prompt = this.buildSummarizationPrompt(messages, interlocutor, language);
      let summary = '';

      if (this.googleModel) {
        const timeoutMs = this.config.timeoutMs ?? Number(process.env.GEMINI_TIMEOUT_MS || 30000);
        const resp: any = await new Promise((resolve, reject) => {
          const timer = setTimeout(() => reject(new Error(`Timeout after ${timeoutMs}ms`)), timeoutMs);
          this.googleModel!.generateContent(prompt)
            .then((r: any) => { clearTimeout(timer); resolve(r); })
            .catch((err: any) => { clearTimeout(timer); reject(err); });
        });
        const textFn = resp?.response?.text;
        const text = typeof textFn === 'function' ? resp.response.text() : resp?.response?.text;
        summary = (text as string) || '';
      } else {
        const response = await this.provider.generateResponse(prompt, 300);
        if (response && response.content) summary = response.content.trim();
      }

      if (summary) {
        const processingTime = Date.now() - startTime;

        console.log(`üìù R√©sum√© g√©n√©r√©: ${summary.length} caract√®res`);

        return {
          summary,
          compressionRatio: this.calculateCompressionRatio(messages, summary),
          qualityScore: this.assessSummaryQuality(summary, messages),
          metadata: {
            originalMessages: messages.length,
            summaryLength: summary.length,
            processingTime
          }
        };
      } else {
        const msg = 'R√©ponse vide du provider';
        console.warn('‚ö†Ô∏è ' + msg);
        if (this.config.strict) {
          throw new Error(msg);
        }
        return this.fallbackSummary(messages, interlocutor, startTime);
      }
    } catch (error) {
      console.error(`‚ùå Erreur g√©n√©ration r√©sum√©: ${error}`);
      if (this.config.strict) {
        throw error;
      }
      return this.fallbackSummary(messages, interlocutor, startTime);
    }
  }

  private async vertexSummarize(
    messages: ConversationMessage[],
    interlocutor: string,
    language: string,
    startTime: number
  ): Promise<SummaryResult> {
    const prompt = this.buildSummarizationPrompt(messages, interlocutor, language);
    const project = this.config.project || process.env.GOOGLE_CLOUD_PROJECT || process.env.PROJECT_ID;
    const location = this.config.location || process.env.VERTEX_LOCATION || process.env.LOCATION || 'us-central1';
    const maxTokens = this.config.maxOutputTokens || 512;
    const timeoutMs = this.config.timeoutMs ?? Number(process.env.GEMINI_TIMEOUT_MS || 30000);
    if (!project) throw new Error('Vertex project is required for L1 Vertex mode');
    const auth = new GoogleAuth({ scopes: ['https://www.googleapis.com/auth/cloud-platform'] });
    const client = await auth.getClient();
    const token = await client.getAccessToken();
    if (!token || !token.token) throw new Error('Failed to obtain Vertex token (L1)');
    const model = (this.config.model || process.env.GEMINI_MODEL || 'gemini-2.5-flash') as string;
    const url = `https://${location}-aiplatform.googleapis.com/v1/projects/${project}/locations/${location}/publishers/google/models/${encodeURIComponent(model)}:generateContent`;
    const body = { contents: [{ role: 'user', parts: [{ text: prompt }] }], generationConfig: { maxOutputTokens: maxTokens, temperature: 0.3 } };
    const ctrl = new AbortController();
    const t = setTimeout(() => ctrl.abort(), timeoutMs);
    try {
      const res = await fetch(url, { method: 'POST', headers: { 'Authorization': `Bearer ${token.token}`, 'Content-Type': 'application/json' }, body: JSON.stringify(body), signal: ctrl.signal as any });
      const js: any = await res.json();
      const text = (js?.candidates?.[0]?.content?.parts || []).map((p: any) => p.text).join('');
      const summary = String(text || '').trim();
      if (!summary) throw new Error('Empty summary from Vertex');
      const processingTime = Date.now() - startTime;
      console.log(`üìù R√©sum√© g√©n√©r√© (Vertex): ${summary.length} caract√®res`);
      return {
        summary,
        compressionRatio: this.calculateCompressionRatio(messages, summary),
        qualityScore: this.assessSummaryQuality(summary, messages),
        metadata: {
          originalMessages: messages.length,
          summaryLength: summary.length,
          processingTime
        }
      };
    } finally {
      clearTimeout(t);
    }
  }

  /**
   * Construit le prompt pour la g√©n√©ration du r√©sum√© narratif
   */
  private buildSummarizationPrompt(
    messages: ConversationMessage[],
    interlocutor: string,
    language: string
  ): string {
    // Formater la conversation
    const conversationText = this.formatConversationForSummary(messages, interlocutor);

    // Utiliser les prompts simples selon la langue
    const prompts = getSimplePrompts(language);
    const personaName = prompts.personaName;
    const personaDescription = prompts.personaDescription;

    const lengthLine = this.config.minSummaryLength && this.config.minSummaryLength > 0
      ? `${this.config.lengthEmphasis ? 'IMPORTANT: ' : ''}Longueur: entre ${this.config.minSummaryLength} et ${this.config.maxSummaryLength} caract√®res`
      : `Maximum ${this.config.maxSummaryLength} caract√®res`;

    const prompt = `Tu es ${personaName} qui r√©sume ses propres conversations.

Personnalit√©: ${personaDescription}
Style: myst√©rieux mais bienveillant, narratif et engageant

T√¢che: Analyse cette conversation et cr√©e un r√©sum√© narratif concis et naturel.

Conversation √† r√©sumer:
${conversationText}

Instructions pour le r√©sum√©:
1. R√©sume en tant que ${personaName} (utilise 'je' et le pr√©nom de l'utilisateur: '${interlocutor}')
2. Cr√©e une histoire naturelle de l'√©volution de la conversation
3. Capture les sujets cl√©s et informations importantes
4. Inclus le contexte de l'utilisateur et ses int√©r√™ts
5. Utilise un style narratif: "${interlocutor} a test√© mes capacit√©s... j'ai r√©pondu..."
6. ${lengthLine}
7. √âcris en fran√ßais naturel et fluide
8. Garde le ton myst√©rieux mais bienveillant de ${personaName}
9. Utilise toujours le pr√©nom "${interlocutor}" au lieu de "tu" pour plus de naturel

R√©sum√© narratif:`;

    return prompt;
  }

  /**
   * Formate la conversation pour le r√©sum√©
   */
  private formatConversationForSummary(
    messages: ConversationMessage[],
    interlocutor: string
  ): string {
    const lines: string[] = [];

    for (const msg of messages) {
      if (msg.role === 'user') {
        lines.push(`${interlocutor}: ${msg.content}`);
      } else {
        lines.push(`Assistant: ${msg.content}`);
      }
    }

    return lines.join('\n');
  }

  /**
   * R√©sum√© de fallback quand le provider n'est pas disponible
   */
  private fallbackSummary(
    messages: ConversationMessage[],
    interlocutor: string,
    startTime: number
  ): SummaryResult {
    const totalMessages = messages.length;
    const topics = this.extractTopicsFallback(messages);

    let summary = `Conversation avec ${interlocutor} (${totalMessages} messages). `;

    if (topics.length > 0) {
      summary += `Sujets abord√©s: ${topics.slice(0, 3).join(', ')}.`;
    }

    const processingTime = Date.now() - startTime;

    return {
      summary,
      compressionRatio: this.calculateCompressionRatio(messages, summary),
      qualityScore: 0.5, // Score de base pour le fallback
      metadata: {
        originalMessages: totalMessages,
        summaryLength: summary.length,
        processingTime
      }
    };
  }

  /**
   * Extraction basique de sujets pour le fallback
   */
  private extractTopicsFallback(messages: ConversationMessage[]): string[] {
    const topics: string[] = [];
    const keywords = [
      'IA', 'intelligence artificielle', 'TFME', 'chat', 'm√©moire', 
      'conversation', 'r√©sum√©', 'agent', 'Algareth', 'LR Hub'
    ];

    for (const msg of messages) {
      const content = msg.content.toLowerCase();
      for (const keyword of keywords) {
        if (content.includes(keyword.toLowerCase()) && !topics.includes(keyword)) {
          topics.push(keyword);
        }
      }
    }

    return topics;
  }

  /**
   * Cr√©e un meta-r√©sum√© des r√©sum√©s - vraie conscience r√©cursive
   */
  async metaSummarize(
    summaries: string[],
    interlocutor: string = 'user',
    language: string = 'fr'
  ): Promise<string> {
    if (!this.provider || summaries.length === 0) {
      return `√âvolution des conversations avec ${interlocutor}: ${summaries.length} sessions r√©sum√©es.`;
    }

    try {
      const summariesText = summaries
        .map((s, i) => `Session ${i + 1}: ${s}`)
        .join('\n\n');

      const prompts = getSimplePrompts(language);
      const personaName = prompts.personaName;

      const prompt = `Tu es ${personaName} qui analyse l'√©volution de ses conversations.

T√¢che: Analyse ces r√©sum√©s de conversations et cr√©e un meta-r√©sum√© de l'√©volution.

R√©sum√©s des sessions:
${summariesText}

Instructions pour le meta-r√©sum√©:
1. Analyse l'√©volution de la relation et des conversations
2. Identifie les patterns d'apprentissage et de d√©veloppement
3. D√©cris comment la compr√©hension mutuelle s'est d√©velopp√©e
4. Mentionne les progr√®s et les d√©couvertes
5. Utilise un style narratif sur l'√©volution temporelle
6. Maximum 300 caract√®res
7. √âcris en tant que ${personaName} (utilise 'je' et le pr√©nom de l'utilisateur: '${interlocutor}')
8. Utilise toujours le pr√©nom "${interlocutor}" au lieu de "tu" pour plus de naturel

Meta-r√©sum√© de l'√©volution:`;

      const response = await this.provider.generateResponse(prompt, 600);

      if (response && response.content) {
        const metaSummary = response.content.trim();
        console.log(`üß† Meta-r√©sum√© g√©n√©r√©: ${metaSummary.length} caract√®res`);
        return metaSummary;
      } else {
        return `√âvolution des conversations avec ${interlocutor}: ${summaries.length} sessions r√©sum√©es.`;
      }
    } catch (error) {
      console.error(`‚ùå Erreur meta-r√©sum√©: ${error}`);
      return `√âvolution des conversations avec ${interlocutor}: ${summaries.length} sessions r√©sum√©es.`;
    }
  }

  /**
   * Calcule le ratio de compression
   */
  private calculateCompressionRatio(messages: ConversationMessage[], summary: string): number {
    const originalLength = messages.reduce((total, msg) => total + msg.content.length, 0);
    return originalLength > 0 ? summary.length / originalLength : 0;
  }

  /**
   * √âvalue la qualit√© du r√©sum√©
   */
  private assessSummaryQuality(summary: string, messages: ConversationMessage[]): number {
    // Score basique bas√© sur la longueur et la coh√©rence
    let score = 0.5;

    // Bonus pour une longueur appropri√©e
    if (summary.length >= 50 && summary.length <= this.config.maxSummaryLength) {
      score += 0.2;
    }

    // Bonus pour la pr√©sence de mots-cl√©s importants
    const keywords = ['tu', 'je', 'conversation', 'discut√©', 'parl√©'];
    const keywordCount = keywords.filter(keyword => 
      summary.toLowerCase().includes(keyword)
    ).length;
    score += (keywordCount / keywords.length) * 0.3;

    return Math.min(score, 1.0);
  }

  /**
   * Obtient les statistiques d'un r√©sum√©
   */
  getSummaryStats(summary: string): {
    length: number;
    wordCount: number;
    sentenceCount: number;
    compressionRatio: number;
  } {
    return {
      length: summary.length,
      wordCount: summary.split(/\s+/).length,
      sentenceCount: (summary.match(/[.!?]+/g) || []).length,
      compressionRatio: summary.length / 1000
    };
  }
}
